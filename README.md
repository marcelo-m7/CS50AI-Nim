# üß† Agente de Q-Learning para o Jogo Nim

**Aluno:** Marcelo Santos (a79433)
**Curso:** Engenharia de Sistemas e Tecnologias Inform√°ticas ‚Äî Universidade do Algarve
**Unidade Curricular:** Intelig√™ncia Artificial

Este projeto tem como objetivo implementar um agente inteligente capaz de aprender autonomamente a jogar o jogo **Nim**, recorrendo a **aprendizagem por refor√ßo** com a t√©cnica de **Q-learning**.

---

## Objetivo Geral

Criar uma intelig√™ncia artificial baseada em **aprendizagem por refor√ßo** (*Reinforcement Learning*), utilizando a t√©cnica de **Q-learning**, para que o agente aprenda estrat√©gias vencedoras no jogo Nim jogando repetidamente contra si mesmo.

---

## O Jogo Nim ‚Äî Problema a Resolver

O Nim consiste em v√°rios montes, cada um com um n√∫mero de objetos.
Em cada jogada:

* O jogador escolhe **um √∫nico monte**
* Retira **quantos objetos quiser** desse monte

**Perde quem retirar o √∫ltimo objeto.**

Apesar da simplicidade, a combina√ß√£o de m√∫ltiplos montes produz um espa√ßo de estados grande, tornando o problema ideal para t√©cnicas de aprendizagem por refor√ßo.

---

## Arquitetura da Solu√ß√£o

O projeto √© composto por dois ficheiros principais:

* **`nim.py`** ‚Äî implementa√ß√£o do jogo e do agente Q-learning
* **`play.py`** ‚Äî interface para treino e jogo humano

### Fun√ß√µes-chave implementadas (resumo)

#### `get_q_value(state, action)`

* Retorna o valor Q associado ao par `(estado, a√ß√£o)`.
* Devolve `0.0` caso ainda n√£o exista Q registado.

#### `update_q_value(state, action, old_q, reward, future_rewards)`

* Aplica a f√≥rmula do Q-learning.
* Atualiza a tabela Q interna (`self.q`).
* Inclui logs √∫teis para debug.

#### `best_future_reward(state)`

* Calcula o maior Q poss√≠vel entre todas as a√ß√µes v√°lidas naquele estado.
* Retorna `0.0` se n√£o existirem valores Q registados.

#### `choose_action(state, epsilon=True)`

* Implementa a pol√≠tica **Œµ-greedy**:

  * Com probabilidade Œµ, escolhe a√ß√£o aleat√≥ria.
  * Caso contr√°rio, escolhe a a√ß√£o com maior valor Q.

---

## Resultados Preliminares do Treino

Treino r√°pido utilizado:

```bash
python3 -c "from nim import train; train(10)"
```

Observa√ß√µes do log:

* No in√≠cio, quase todos os `best_future_reward` s√£o `0.0` (tabela Q vazia).
* Com algumas itera√ß√µes, surgem valores positivos crescentes (`0.25`, `0.5`, `0.75`, `0.875`) ‚Äî o agente refor√ßa decis√µes boas.
* Surgem tamb√©m valores negativos (`-0.5`, `-0.75`, `-0.96875`) ‚Äî puni√ß√µes propagadas de jogadas que levaram √† derrota.
* A tabela Q come√ßa a ganhar forma, distinguindo movimentos vantajosos dos prejudiciais.

**Interpreta√ß√£o:** o agente demonstra sinais claros de aprendizagem ‚Äî ap√≥s mais jogos, espera-se estabiliza√ß√£o das estrat√©gias.

---

## Componentes Implementados

### 1. **Classe `Nim`**

* Representa o estado do jogo.
* Gera a√ß√µes v√°lidas.
* Aplica jogadas e alterna turnos.

### 2. **Classe `NimAI`**

* Gera√ß√£o e atualiza√ß√£o de valores Q.
* Estimativa de recompensas futuras.
* Sele√ß√£o de a√ß√µes com pol√≠tica Œµ-greedy.

### 3. **Treino e Jogo**

* Treino aut√≥nomo (self-play).
* Possibilidade de jogar contra o agente ap√≥s treino.

---

## Metodologia (Q-Learning)

O agente segue a atualiza√ß√£o:

```
Q(s,a) ‚Üê old_q + Œ± * ((reward + future_reward) ‚àí old_q)
```

Onde:

* **s** = estado atual
* **a** = a√ß√£o tomada
* **Œ±** = taxa de aprendizagem
* **reward** = recompensa imediata
* **future_reward** = melhor Q futuro poss√≠vel

Com repeti√ß√£o suficiente, o agente ajusta os seus Q-values at√© convergir para uma pol√≠tica est√°vel.

---

## Como Utilizar

### Treino

```bash
python nim.py
```

### Jogo humano vs agente

```bash
python play.py
```

### Treino r√°pido para inspe√ß√£o

```bash
python -c "from nim import train; train(10)"
```

### Ambiente virtual (opcional)

```bash
python -m venv venv
```

---

## Ferramentas e T√©cnicas

* Python 3.12
* Algoritmo **Q-learning**
* Pol√≠tica **Œµ-greedy**
* Testes autom√°ticos com CS50 (`check50`, `style50`, `submit50`)
* Git e documenta√ß√£o estruturada

---

## Melhorias Futuras (Features Planeadas)

1. Persist√™ncia da tabela Q (`pickle`) para evitar re-treino completo.
2. Experimentos com diferentes hiperpar√¢metros (Œ±, Œµ, n¬∫ de jogos) incluindo registo de win-rate.
3. Testes unit√°rios para fun√ß√µes essenciais (`get_q_value`, `update_q_value`, `choose_action`, etc.).

---

## Verifica√ß√µes

![style50](docs/style50.png)

![check50](docs/check50.png)

![submit50](docs/submit50.png)

---

## üîó Refer√™ncias

- [Nim ‚Äì CS50's Introduction to AI](https://cs50.harvard.edu/ai/projects/4/nim/)
- [Neural Networks ‚Äì Lecture 5 (CS50 AI 2020)](https://youtu.be/J1QD9hLDEDY?si=41EOOXi-BaDbVy5E)

---

**Universidade do Algarve ‚Äî Departamento de Engenharia Eletrot√©cnica e Inform√°tica**
*2025*

---
