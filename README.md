# ğŸ§  Agente de Q-Learning para o Jogo Nim

**Aluno:** Marcelo Santos (a79433)
**Curso:** Engenharia de Sistemas e Tecnologias InformÃ¡ticas â€” Universidade do Algarve
**Unidade Curricular:** InteligÃªncia Artificial

Este projeto tem como objetivo implementar um agente inteligente capaz de aprender autonomamente a jogar o jogo **Nim**, recorrendo a **aprendizagem por reforÃ§o** com a tÃ©cnica de **Q-learning**.

---

## ğŸ¯ Objetivo Geral

Criar uma inteligÃªncia artificial baseada em **aprendizagem por reforÃ§o** (*Reinforcement Learning*), utilizando a tÃ©cnica de **Q-learning**, para que o agente aprenda estratÃ©gias vencedoras no jogo Nim jogando repetidamente contra si mesmo.

---

## ğŸ® O Jogo Nim â€” Problema a Resolver

O Nim consiste em vÃ¡rios montes, cada um com um nÃºmero de objetos.
Em cada jogada:

* O jogador escolhe **um Ãºnico monte**
* Retira **quantos objetos quiser** desse monte

**Perde quem retirar o Ãºltimo objeto.**

Apesar da simplicidade, a combinaÃ§Ã£o de mÃºltiplos montes produz um espaÃ§o de estados grande, tornando o problema ideal para tÃ©cnicas de aprendizagem por reforÃ§o.

---

## ğŸ§© Arquitetura da SoluÃ§Ã£o

O projeto Ã© composto por dois ficheiros principais:

* **`nim.py`** â€” implementaÃ§Ã£o do jogo e do agente Q-learning
* **`play.py`** â€” interface para treino e jogo humano

### FunÃ§Ãµes-chave implementadas (resumo)

#### `get_q_value(state, action)`

* Retorna o valor Q associado ao par `(estado, aÃ§Ã£o)`.
* Devolve `0.0` caso ainda nÃ£o exista Q registado.

#### `update_q_value(state, action, old_q, reward, future_rewards)`

* Aplica a fÃ³rmula do Q-learning.
* Atualiza a tabela Q interna (`self.q`).
* Inclui logs Ãºteis para debug.

#### `best_future_reward(state)`

* Calcula o maior Q possÃ­vel entre todas as aÃ§Ãµes vÃ¡lidas naquele estado.
* Retorna `0.0` se nÃ£o existirem valores Q registados.

#### `choose_action(state, epsilon=True)`

* Implementa a polÃ­tica **Îµ-greedy**:

  * Com probabilidade Îµ, escolhe aÃ§Ã£o aleatÃ³ria.
  * Caso contrÃ¡rio, escolhe a aÃ§Ã£o com maior valor Q.

---

## ğŸ“Š Resultados Preliminares do Treino

Treino rÃ¡pido utilizado:

```bash
python3 -c "from nim import train; train(10)"
```

ObservaÃ§Ãµes do log:

* No inÃ­cio, quase todos os `best_future_reward` sÃ£o `0.0` (tabela Q vazia).
* Com algumas iteraÃ§Ãµes, surgem valores positivos crescentes (`0.25`, `0.5`, `0.75`, `0.875`) â€” o agente reforÃ§a decisÃµes boas.
* Surgem tambÃ©m valores negativos (`-0.5`, `-0.75`, `-0.96875`) â€” puniÃ§Ãµes propagadas de jogadas que levaram Ã  derrota.
* A tabela Q comeÃ§a a ganhar forma, distinguindo movimentos vantajosos dos prejudiciais.

**InterpretaÃ§Ã£o:** o agente demonstra sinais claros de aprendizagem â€” apÃ³s mais jogos, espera-se estabilizaÃ§Ã£o das estratÃ©gias.

---

## ğŸ—ï¸ Componentes Implementados

### 1. **Classe `Nim`**

* Representa o estado do jogo.
* Gera aÃ§Ãµes vÃ¡lidas.
* Aplica jogadas e alterna turnos.

### 2. **Classe `NimAI`**

* GeraÃ§Ã£o e atualizaÃ§Ã£o de valores Q.
* Estimativa de recompensas futuras.
* SeleÃ§Ã£o de aÃ§Ãµes com polÃ­tica Îµ-greedy.

### 3. **Treino e Jogo**

* Treino autÃ³nomo (self-play).
* Possibilidade de jogar contra o agente apÃ³s treino.

---

## ğŸ“ Metodologia (Q-Learning)

O agente segue a atualizaÃ§Ã£o:

```
Q(s,a) â† old_q + Î± * ((reward + future_reward) âˆ’ old_q)
```

Onde:

* **s** = estado atual
* **a** = aÃ§Ã£o tomada
* **Î±** = taxa de aprendizagem
* **reward** = recompensa imediata
* **future_reward** = melhor Q futuro possÃ­vel

Com repetiÃ§Ã£o suficiente, o agente ajusta os seus Q-values atÃ© convergir para uma polÃ­tica estÃ¡vel.

---

## â–¶ï¸ Como Utilizar

### Treino

```bash
python nim.py
```

### Jogo humano vs agente

```bash
python play.py
```

### Treino rÃ¡pido para inspeÃ§Ã£o

```bash
python -c "from nim import train; train(10)"
```

### Ambiente virtual (opcional)

```bash
python -m venv venv
```

---

## ğŸ› ï¸ Ferramentas e TÃ©cnicas

* Python 3.12
* Algoritmo **Q-learning**
* PolÃ­tica **Îµ-greedy**
* Testes automÃ¡ticos com CS50 (`check50`, `style50`, `submit50`)
* Git e documentaÃ§Ã£o estruturada

---

## ğŸš€ Melhorias Futuras (Features Planeadas)

1. PersistÃªncia da tabela Q (`pickle`) para evitar re-treino completo.
2. Experimentos com diferentes hiperparÃ¢metros (Î±, Îµ, nÂº de jogos) incluindo registo de win-rate.
3. Testes unitÃ¡rios para funÃ§Ãµes essenciais (`get_q_value`, `update_q_value`, `choose_action`, etc.).

---

## ğŸ”— ReferÃªncias

- [Nim â€“ CS50's Introduction to AI](https://cs50.harvard.edu/ai/projects/4/nim/)
- [Neural Networks â€“ Lecture 5 (CS50 AI 2020)](https://youtu.be/J1QD9hLDEDY?si=41EOOXi-BaDbVy5E)

---

**Universidade do Algarve â€” Departamento de Engenharia EletrotÃ©cnica e InformÃ¡tica**
*2025*

---